Company: IBM Italy & Politecnico di Milano

Date: 1/06/19

Supervisors: Prof. Stefano Zanero (Politecnico), Michele Rosati (IBM)

Author: Niccolò Howard Minetti


Neural Log Classifier  - 2.0


Introduction

Implementation of IBM QRadar custom parsers can take up to 3 weeks of work when client infrastructure does not use standard systems (systems that already have a parser). To optimise and reduce deployment time the objective of this project is to build a lightweight neural network that takes in as input raw “utf_payload” along with the “category_categoryname” (either high or low level). The network is trained on pre-labeled log data from a given deployment. The project will consist of a simple two part algorithm, one for training and testing, while the other for lightweight classification.
Approach

Our approach is divided into 2 main processes. The first consists in collecting, cleaning and feeding a homogeneous dataset to the neural net for training and testing. The second part samples a random logs set to train and test the network to see its capabilities in predicting category from raw utf_payload. Random sampling has been added to make sure that high efficiency is not an outlier. 
Cleaning and Log preparation

Given a CSV file than has a collection of logs exported from IBM QRadar we used the Pandas Library to sort, organise and select a homogeneous sample of logs that belong to a rich set of high level categories, threshold set to 999 elements of same category, and saving them using the pickle library. With the data given, we managed to extract 7 macro categories with 32 micro categories in a rich dataset.
Preprocessing and Tokenization

Before we can train the neural network we need to preprocess the logs. To do so we run all our sample through a tokenizer that extracts a dictionary of the 500 most used “words” and tokenizes each log to our newly extracted dictionary. The tokenizer built on this dataset is stored so it can be used to preprocess future unlabelled logs with the same set of words.
Neural Net

A different series of approaches were applied to this problem and the result was the following neural net. The neural network, built with Keras with Tensorflow backend, is a two layer Sequential classifier that takes as input a 500 word dictionary encoding of each log. The first layer has 512 neurons and outputs a tensor of the same shape of input. The first layer uses a ReLu (Rectified Linear Unit) as its activation function. The second layer has an input of 512 elements while having only a number of neurons equal to the number of distinct categories found in the sampled dataset. The second layer uses Softmax as its activation function. The Loss model is Categorical Crossentropy (also known as Softmax Loss) this allows us to output a probability over the classes for each log. The optimizer used is Adam (from adaptive moment estimation) ”… an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.”. Our metric will be accuracy.


Code, Insights and Results

This algorithm shows results across the board. High level classification has been achieved with 30 to 32 classes with over 98% accuracy. Low level classification has been achieved 95% accuracy as can be seen below.
Not all high and low level classes have been uses due to scarcity of logs belonging to that class. 
All code has been uploaded along with this report.

Next Step

To truly uncover the potential of this project the next steps to take are:
A standard dataset of logs generated by Security Events in a Network
A user-friendly front end interface for parsing
Insight capabilities that show why a new log would be placed is a certain category (such as neutron activation tracking)
Linking new uncategorised logs with the classifier to better connect unexpected activity with Security event management.
