{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company: IBM Italy & Politecnico di Milano\n",
    "# Last update: 30 May 2019\n",
    "# Author: Niccolo Howard Minetti\n",
    "# Version: 2.0 \n",
    "# Notes: This script pools a good set of logs for training and testing the model and saves Tokenizer and Model.\n",
    "# ID: net_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd     #Pandas Library for data manipulation\n",
    "import numpy as np     #Numpy Library for array operations\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder #SKLearn Library for creating Dictionary and Encoder\n",
    "\n",
    "#ML framework on top of Tensorflow\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential, model_from_json, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import utils\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'categoryname_category' #Insert 'categoryname_category' or 'categoryname_highlevelcategory'\n",
    "data = pd.read_pickle(\"logs/[INSERT_SAMPLED_LOGS_FILE_NAME]\")         #Load log extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 32 Categories \n",
      "\n",
      "Train on 3600 samples, validate on 400 samples\n",
      "Epoch 1/2\n",
      "3600/3600 [==============================] - 3s 805us/step - loss: 0.4128 - acc: 0.9044 - val_loss: 0.2418 - val_acc: 0.9525\n",
      "Epoch 2/2\n",
      "3600/3600 [==============================] - 2s 673us/step - loss: 0.1232 - acc: 0.9772 - val_loss: 0.1713 - val_acc: 0.9800\n",
      "1000/1000 [==============================] - 0s 137us/step\n",
      "Test score: 0.1392168325781822\n",
      "Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "labelNum = 0   \n",
    "collect_acc = []\n",
    "    \n",
    "while(labelNum != 32):                                  #Set number of categories to find in random log extraction\n",
    "    data = data.sample(frac=1).reset_index(drop=True)  #Shuffle elements\n",
    "    train_size = int(len(data) * .8)                  #80-20 division for training-testing\n",
    "    train_rLogs = data['utf8_payload'][:train_size]\n",
    "    train_cat = data[label][:train_size]\n",
    "    \n",
    "    labelNum = labelNum = len(train_cat.value_counts())\n",
    "\n",
    "test_rLogs = data['utf8_payload'][train_size:]\n",
    "test_cat = data[label][train_size:]\n",
    "\n",
    "                                                    #Counting the number of labels in the subset\n",
    "print(\"Found:\", labelNum, \"Categories \\n\")\n",
    "\n",
    "vocab_size = 500                                  #Size of dictionary\n",
    "tokenize = text.Tokenizer(num_words=vocab_size)   #init Tokenizer\n",
    "tokenize.fit_on_texts(train_rLogs)                #Fit utf_payload to dictionary\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_rLogs)   #Maps logs to matrix\n",
    "x_test = tokenize.texts_to_matrix(test_rLogs)\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_cat)                            #Fits labels for training\n",
    "\n",
    "y_train = encoder.transform(train_cat)\n",
    "y_test = encoder.transform(test_cat)\n",
    "\n",
    "#Linear stack of layers\n",
    "\n",
    "model = []\n",
    "model = Sequential()\n",
    "\n",
    "#First Layer\n",
    "\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))    #Dense layers applies y =ax+b with a Activation function \n",
    "                                                    #with 512 neurons and an input the size of our dictionary\n",
    "\n",
    "model.add(Activation('relu'))                       #ReLu: Rectified Linear Unit (Activation function)\n",
    "\n",
    "#Second Layer\n",
    "\n",
    "model.add(Dense(labelNum))\n",
    "\n",
    "\n",
    "model.add(Activation('softmax'))                    #Softmax activation function\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',      #Categorical CrossEntropy is CrossEntropy with Softmax\n",
    "              optimizer='adam',                     #Adaptive moment estimation for gradient optimisation\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,               #Trains model with 32 logs at a time\n",
    "                    batch_size=32, \n",
    "                    epochs=2, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test,              #Evaluation of model\n",
    "                       batch_size=32, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "collect_acc.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/NLP2.0/model/logModel.h5\"\n",
    "model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenize.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenize, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Access Denied', 'Access Permitted', 'Auth Server Session Closed',\n",
       "       'Compliance Policy Violation', 'Error', 'File Transfer',\n",
       "       'Firewall Deny', 'Firewall Permit',\n",
       "       'General Authentication Failed', 'Host Login Succeeded',\n",
       "       'IRC Policy Violation', 'Information', 'Kerberos Session Denied',\n",
       "       'Kerberos Session Opened', 'Messages', 'Misc Login Succeeded',\n",
       "       'Misc Logout', 'Object Cached', 'Object Not Cached',\n",
       "       'Privilege Escalation Succeeded', 'RADIUS Session Ended',\n",
       "       'RADIUS Session Status', 'SSH Closed', 'Service Stopped',\n",
       "       'Session Closed', 'Session Opened', 'System Action Allow',\n",
       "       'System Status', 'User Login Failure', 'User Login Success',\n",
       "       'User Right Assigned', 'Warning'], dtype='<U30')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
